{
 "metadata": {
  "name": "gradient_analysis_workflow"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Set up workflow parameters"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Configure the variables in this section to control how the workflow will be executed. For example, what studies to analyze, categories of interest, etc.."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "test = False\n",
      "\n",
      "if test:\n",
      "    in_dir = 'test_datasets'\n",
      "    out_dir = 'test_gradient_analysis_output'\n",
      "    tree_fp = join('test_datasets', 'overview', 'rep_set.tre')\n",
      "    depth_descs = ['5_percent', '25_percent', '50_percent']\n",
      "    studies = {\n",
      "               'overview': {\n",
      "                            'depths': [50, 100, 146],\n",
      "                            'categories': ['DOB'],\n",
      "                            'subsets': [3, 4],\n",
      "                            'best_method_env_vars': ['DOB']\n",
      "                           }\n",
      "              }\n",
      "    metrics = ['euclidean', 'bray_curtis']\n",
      "    methods = {\n",
      "               'mantel': parse_mantel_results,\n",
      "               'mantel_corr': None,\n",
      "               'morans_i': parse_morans_i_results,\n",
      "               'partial_mantel': parse_partial_mantel_results,\n",
      "              }\n",
      "    permutations = [99, 999]\n",
      "    num_shuffled = 2\n",
      "    num_subsets = 2\n",
      "else:\n",
      "    in_dir = 'datasets'\n",
      "    out_dir = 'gradient_analysis_output'\n",
      "    tree_fp = join('gg_otus_4feb2011', 'trees', 'gg_97_otus_4feb2011.tre')\n",
      "    depth_descs = ['5_percent', '25_percent', '50_percent']\n",
      "    studies = {\n",
      "               '88_soils': {\n",
      "                            'depths': [400, 580, 660],\n",
      "                            'categories': ['LATITUDE', 'PH'],\n",
      "                            'subsets': [10, 20, 30],\n",
      "                            'best_method_env_vars': ['TOT_ORG_CARB', 'SILT_CLAY', 'ELEVATION', 'SOIL_MOISTURE_DEFICIT', 'CARB_NITRO_RATIO',\n",
      "                                                     'ANNUAL_SEASON_TEMP', 'ANNUAL_SEASON_PRECPT', 'PH', 'CMIN_RATE', 'LONGITUDE', 'LATITUDE']\n",
      "                           }, \n",
      "               'glen_canyon': {\n",
      "                               'depths': [15000, 29000, 53000],\n",
      "                               'categories': ['estimated_years_since_submerged_for_plotting'],\n",
      "                               'subsets': [10, 20, 30],\n",
      "                               'best_method_env_vars': ['sample_pH', 'estimated_years_since_submerged_for_plotting', 'Month', 'Day', 'Year',\n",
      "                                                        'days_since_epoch', 'Hour', 'Replicate', 'DNA.I.D.No.']\n",
      "                              },\n",
      "               'keyboard': {\n",
      "                            'depths': [390, 780, 1015],\n",
      "                            'categories': [],\n",
      "                            'subsets': [10, 20, 30],\n",
      "                            'best_method_env_vars': []\n",
      "                           }\n",
      "              }\n",
      "    metrics = ['euclidean', 'bray_curtis', 'weighted_unifrac', 'unweighted_unifrac']\n",
      "    methods = {\n",
      "               'mantel': parse_mantel_results,\n",
      "               'mantel_corr': None,\n",
      "               'morans_i': parse_morans_i_results,\n",
      "               'partial_mantel': parse_partial_mantel_results\n",
      "              }\n",
      "    permutations = [99, 999]\n",
      "    num_shuffled = 5\n",
      "    num_subsets = 5"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 29
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Run BEST method"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "BEST is very different from the rest of the methods in terms of input and output, so run it separately here."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def run_per_study_depth_best_analysis((study, depth, env_vars, out_dir)):\n",
      "    from os.path import basename, join, splitext\n",
      "    \n",
      "    if len(env_vars) > 0:\n",
      "        study_dir = join(out_dir, study)\n",
      "        map_fp = join(study_dir, 'map.txt')\n",
      "        depth_dir = join(study_dir, 'bdiv_even%d' % depth)\n",
      "        dm_wildcard = join(depth_dir, '*_dm*.txt')\n",
      "        dm_fps = !ls $dm_wildcard\n",
      "        env_vars_str = ','.join(env_vars)\n",
      "        \n",
      "        for dm_fp in dm_fps:\n",
      "            results_dir = join(depth_dir, '%s_%s' % (splitext(basename(dm_fp))[0], 'best'))\n",
      "            !compare_categories.py --method 'best' -i $dm_fp -m $map_fp -c $env_vars_str -o $results_dir\n",
      "\n",
      "# Process BEST at each depth in each study in parallel.\n",
      "c = Client()\n",
      "dview = c[:]\n",
      "dview.block = True\n",
      "\n",
      "per_study_depths = []\n",
      "for study in studies:\n",
      "    for depth in studies[study]['depths']:\n",
      "        per_study_depths.append((study, depth, studies[study]['best_method_env_vars'], out_dir))\n",
      "out = dview.map(run_per_study_depth_best_analysis, per_study_depths)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Gradient subset testing"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Specifically test Mantel and Moran's I on subsets of the original distance matrix. This time the subsets will not be randomly chosen, but instead samples will be chosen along the gradient for each subset (which is what a researcher might do instead of randomly picking samples in order to test a gradient). A plot will be generated with subset size on the x-axis and test statistic on the y-axis. This will allow us to see if there is a cutoff/threshold for gradient detection based on the number of samples."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from __future__ import division\n",
      "from math import ceil\n",
      "from os.path import join\n",
      "from random import randint, shuffle\n",
      "from matplotlib.pyplot import errorbar, figure, legend, title, xlabel, xlim, ylabel\n",
      "from numpy import mean, std\n",
      "from qiime.filter import filter_samples_from_distance_matrix\n",
      "from qiime.parse import parse_distmat, parse_mapping_file_to_dict\n",
      "\n",
      "def run_cmd(cmd):\n",
      "    !$cmd\n",
      "\n",
      "samp_sizes = [5, 10, 20, 40, 60, 80]\n",
      "num_subsets = 10\n",
      "study = '88_soils'\n",
      "gradients = {'PH': ['b', 'pH'], 'LATITUDE': ['r', 'Latitude']}\n",
      "even_depth = 400\n",
      "out_dir = '%s_gradient_analysis' % study\n",
      "methods = {\n",
      "           'mantel': parse_mantel_results,\n",
      "           'morans_i': parse_morans_i_results\n",
      "          }\n",
      "map_fp = 'gradient_analysis_output/%s/map.txt' % study\n",
      "num_perms = 999"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 17
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "mdm, _ = parse_mapping_file_to_dict(open(map_fp, 'U'))\n",
      "dm_labels, dm_data = parse_distmat(open('gradient_analysis_output/%s/bdiv_even%d/unweighted_unifrac_dm.txt' % (study, even_depth), 'U'))\n",
      "!mkdir $out_dir\n",
      "\n",
      "cmds = []\n",
      "for gradient, plot_options in gradients.items():\n",
      "    # Only keep the sample IDs that are in both the mapping file and distance matrix.\n",
      "    samp_ids = [(samp_id, float(metadata[gradient])) for samp_id, metadata in mdm.items() if samp_id in dm_labels]\n",
      "    samp_ids.sort(key=lambda samp_id: samp_id[1])\n",
      "    \n",
      "    for samp_size in samp_sizes:\n",
      "        # Adapted from http://stackoverflow.com/a/9873935\n",
      "        # We add 1 to the number of samples we want because we want samp_size intervals to choose from.\n",
      "        bin_idxs = [int(ceil(i * len(samp_ids) / (samp_size + 1))) for i in range(samp_size + 1)]\n",
      "        \n",
      "        for subset_num in range(1, num_subsets + 1):\n",
      "            samp_ids_to_keep = []\n",
      "            for i in range(len(bin_idxs) - 1):\n",
      "                if i == len(bin_idxs) - 2:\n",
      "                    # We're at the last bin, so choose from the entire bin range.\n",
      "                    samp_ids_to_keep.append(samp_ids[randint(bin_idxs[i], bin_idxs[i + 1])][0])\n",
      "                else:\n",
      "                    # We subtract 1 since randint is inclusive on both sides, and we don't want to choose\n",
      "                    # the same sample ID multiple times from different bins.\n",
      "                    samp_ids_to_keep.append(samp_ids[randint(bin_idxs[i], bin_idxs[i + 1] - 1)][0])\n",
      "            assert len(samp_ids_to_keep) == samp_size, \"%d != %d\" % (len(samp_ids_to_keep), samp_size)\n",
      "            \n",
      "            subset_dm_fp = join(out_dir, 'unweighted_unifrac_dm_%s_size_%d_%d.txt' % (gradient, samp_size, subset_num))\n",
      "            subset_dm_f = open(subset_dm_fp, 'w')\n",
      "            subset_dm_f.write(filter_samples_from_distance_matrix((dm_labels, dm_data), samp_ids_to_keep, negate=True))\n",
      "            subset_dm_f.close()\n",
      "            \n",
      "            # Build Mantel command.\n",
      "            in_dm_fps = '%s,%s' % (subset_dm_fp, join('gradient_analysis_output/%s/%s_dm.txt' % (study, gradient)))\n",
      "            results_dir = join(out_dir, 'unweighted_unifrac_dm_%s_size_%d_%d_mantel' % (gradient, samp_size, subset_num))\n",
      "            cmds.append('compare_distance_matrices.py --method mantel -n %d -i %s -o %s' % (num_perms, in_dm_fps, results_dir))\n",
      "            \n",
      "            # Build Moran's I command.\n",
      "            results_dir = join(out_dir, 'unweighted_unifrac_dm_%s_size_%d_%d_morans_i' % (gradient, samp_size, subset_num))\n",
      "            cmds.append('compare_categories.py --method morans_i -i %s -m %s -c %s -o %s' % (subset_dm_fp, map_fp, gradient, results_dir))\n",
      "\n",
      "c = Client()\n",
      "dview = c[:]\n",
      "dview.block = True\n",
      "shuffle(cmds)\n",
      "out = dview.map(run_cmd, cmds)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 18
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for method, parse_fn in methods.items():\n",
      "    # Twin y-axis code is based on http://matplotlib.org/examples/api/two_scales.html\n",
      "    fig = figure()\n",
      "    ax1 = fig.add_subplot(111)\n",
      "    ax2 = ax1.twinx()\n",
      "    \n",
      "    for gradient, plot_options in gradients.items():\n",
      "        avg_test_stats = []\n",
      "        std_test_stats = []\n",
      "        avg_p_vals = []\n",
      "        std_p_vals = []\n",
      "        \n",
      "        for samp_size in samp_sizes:\n",
      "            test_stats = []\n",
      "            p_vals = []\n",
      "            \n",
      "            for subset_num in range(1, num_subsets + 1):\n",
      "                results_dir = join(out_dir, 'unweighted_unifrac_dm_%s_size_%d_%d_%s' % (gradient, samp_size, subset_num, method))\n",
      "                test_stat, p_val = parse_fn(open(join(results_dir, '%s_results.txt' % method), 'U'))\n",
      "                test_stats.append(test_stat)\n",
      "                p_vals.append(p_val)\n",
      "                \n",
      "            avg_test_stats.append(mean(test_stats))\n",
      "            std_test_stats.append(std(test_stats))\n",
      "            avg_p_vals.append(mean(p_vals))\n",
      "            std_p_vals.append(std(p_vals))\n",
      "        \n",
      "        # Plot test statistics on left axis.\n",
      "        ax1.errorbar(samp_sizes, avg_test_stats, yerr=std_test_stats, color=plot_options[0], label=plot_options[1], fmt='-')\n",
      "        \n",
      "        # Plot p-values on the right axis.\n",
      "        ax2.errorbar(samp_sizes, avg_p_vals, yerr=std_p_vals, color=plot_options[0], label=plot_options[1], fmt='-', linestyle='--')\n",
      "    \n",
      "    xlim(0, 85)\n",
      "    #ax2.set_ylim(0.0, 1.0)\n",
      "    title('%s: %s' % (study, method))\n",
      "    ax1.set_xlabel('Number of samples')\n",
      "    ax1.set_ylabel('Average test statistic with standard deviation')\n",
      "    ax2.set_ylabel('Average p-value with standard deviation')\n",
      "    \n",
      "    #lines, labels = ax1.get_legend_handles_labels()\n",
      "    #lines2, labels2 = ax2.get_legend_handles_labels()\n",
      "    #ax2.legend(lines + lines2, labels + labels2)\n",
      "    legend()\n",
      "    fig.savefig(join(out_dir, 'gradient_analysis_plot_%s_%s.pdf' % (study, method)), format='pdf')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 27
    }
   ],
   "metadata": {}
  }
 ]
}