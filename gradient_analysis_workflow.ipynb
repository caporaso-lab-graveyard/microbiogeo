{
 "metadata": {
  "name": "gradient_analysis_workflow"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Define helper functions"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Define various helper functions and their associated tests that will be used in the workflow (and that are not already in QIIME). Run this cell to ensure all required dependencies are installed and setup correctly (e.g. QIIME, numpy) and that all tests pass before running the workflow."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from os.path import join\n",
      "from random import sample, shuffle\n",
      "\n",
      "from qiime.filter import filter_samples_from_distance_matrix\n",
      "from qiime.format import format_distance_matrix\n",
      "from qiime.parse import parse_distmat\n",
      "\n",
      "# Define functions for manipulating distance matrices.\n",
      "def shuffle_dm(dm_f):\n",
      "    labels, dm_data = parse_distmat(dm_f)\n",
      "    shuffle(labels)\n",
      "    return format_distance_matrix(labels, dm_data)\n",
      "\n",
      "def pick_dm_subset(dm_f, num_samps):\n",
      "    labels, dm_data = parse_distmat(dm_f)\n",
      "    samp_ids_to_keep = sample(labels, num_samps)\n",
      "    return filter_samples_from_distance_matrix((labels, dm_data), samp_ids_to_keep, negate=True)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Set up workflow parameters"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Configure the variables in this section to control how the workflow will be executed. For example, what studies to analyze, categories of interest, etc.."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "test = True\n",
      "\n",
      "if test:\n",
      "    in_dir = 'test_datasets'\n",
      "    out_dir = 'test_gradient_analysis_output'\n",
      "    tree_fp = join('test_datasets', 'overview', 'rep_set.tre')\n",
      "    depth_descs = ['5_percent', '25_percent', '50_percent']\n",
      "    studies = {\n",
      "               'overview': {\n",
      "                            'depths': [50, 100, 146],\n",
      "                            'categories': ['DOB'],\n",
      "                            'subsets': [3, 4]\n",
      "                           }\n",
      "              }\n",
      "    metrics = ['euclidean', 'bray_curtis']\n",
      "    #metrics = ['euclidean', 'bray_curtis', 'weighted_unifrac', 'unweighted_unifrac']\n",
      "    permutations = [99, 999]\n",
      "    num_shuffled = 2\n",
      "    num_subsets = 2\n",
      "    jobs_to_start = 2\n",
      "else:\n",
      "    in_dir = 'datasets'\n",
      "    out_dir = 'gradient_analysis_output'\n",
      "    tree_fp = join('gg_otus_4feb2011', 'trees', 'gg_97_otus_4feb2011.tre')\n",
      "    depth_descs = ['5_percent', '25_percent', '50_percent']\n",
      "    studies = {\n",
      "               '88_soils': {\n",
      "                            'depths': [400, 580, 660],\n",
      "                            'categories': ['LATITUDE', 'PH'],\n",
      "                            'subsets': [5, 10, 20, 40]\n",
      "                           }, \n",
      "               'glen_canyon': {\n",
      "                               'depths': [15000, 29000, 53000],\n",
      "                               'categories': ['estimated_years_since_submerged_for_plotting'],\n",
      "                               'subsets': [5, 10, 20, 40]\n",
      "                              },\n",
      "               'keyboard': {\n",
      "                            'depths': [390, 780, 1015],\n",
      "                            'categories': [],\n",
      "                            'subsets': [5, 10, 20, 40]\n",
      "                           }\n",
      "              }\n",
      "    metrics = ['euclidean', 'bray_curtis', 'weighted_unifrac', 'unweighted_unifrac']\n",
      "    permutations = [99, 999, 9999]\n",
      "    num_shuffled = 3\n",
      "    num_subsets = 3\n",
      "    jobs_to_start = 2"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 8
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Generate distance matrices"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Generate distance matrices for each study at even sampling depths that exclude 5%, 25%, and 50% of the samples from the input OTU table (these numbers were calculated beforehand). Generate Euclidean, Bray-Curtis, weighted UniFrac, and unweighted UniFrac distance matrices at each sampling depth using the GreenGenes 97% tree. Also generate several shuffled versions of each distance matrix, which can be used later as negative controls.\n",
      "\n",
      "In addition, generate several subsets of each distance matrix with the specified number of samples, which can be used later to test how the methods perform on different study sizes.\n",
      "\n",
      "The keyboard study is a bit of an exception in that we only want to create a distance matrix that includes samples taken directly from keys (not human subject fingertips) because we want to see if keys that are closer to each other are correlated with community similarity."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!mkdir $out_dir\n",
      "for study in studies:\n",
      "    in_study_dir = join(in_dir, study)\n",
      "    out_study_dir = join(out_dir, study)\n",
      "    !mkdir $out_study_dir\n",
      "    !cp $in_study_dir/map.txt $out_study_dir/\n",
      "    map_fp = join(out_study_dir, 'map.txt')\n",
      "    \n",
      "    for depth in studies[study]['depths']:\n",
      "        full_otu_fp = join(in_study_dir, 'otu_table.biom')\n",
      "        even_otu_fp = join(out_study_dir, 'otu_table_even%d.biom' % depth)\n",
      "        bdiv_out_dir = join(out_study_dir, 'bdiv_even%d' % depth)\n",
      "        \n",
      "        metrics_param = ','.join(metrics)\n",
      "        !single_rarefaction.py -i $full_otu_fp -o $even_otu_fp -d $depth\n",
      "        !parallel_beta_diversity.py -i $even_otu_fp -o $bdiv_out_dir -m $metrics_param -T -t $tree_fp -O $jobs_to_start\n",
      "        \n",
      "        # Rename each file to match QIIME's standard naming conventions of distance matrices. Generate shuffled versions of each distance matrix.\n",
      "        for metric in metrics:\n",
      "            dm_fp = join(bdiv_out_dir, '%s_otu_table_even%d.txt' % (metric, depth))\n",
      "            renamed_dm_fp = join(bdiv_out_dir, '%s_dm.txt' % metric)\n",
      "            \n",
      "            # Filter the keyboard study distance matrix to include only samples taken from keys of subjects M2, M3, and M9.\n",
      "            if study == 'keyboard':\n",
      "                !filter_distance_matrix.py -i $dm_fp -o $renamed_dm_fp -m $map_fp -s 'COMMON_NAME:keyboard;HOST_SUBJECT_ID:M2,M3,M9'\n",
      "                !rm $dm_fp\n",
      "            else:\n",
      "                !mv $dm_fp $renamed_dm_fp\n",
      "            \n",
      "            for i in range(1, num_shuffled + 1):\n",
      "                renamed_dm_f = open(renamed_dm_fp, 'U')\n",
      "                shuffled_dm_fp = join(bdiv_out_dir, '%s_dm_shuffled%d.txt' % (metric, i))\n",
      "                shuffled_dm_f = open(shuffled_dm_fp, 'w')\n",
      "                shuffled_dm_f.write(shuffle_dm(renamed_dm_f))\n",
      "                shuffled_dm_f.close()\n",
      "                renamed_dm_f.close()\n",
      "            \n",
      "            # Create subsets of each non-shuffled distance matrix.\n",
      "            for subset in studies[study]['subsets']:\n",
      "                for i in range(1, num_subsets + 1):\n",
      "                        subset_dm_fp = join(bdiv_out_dir, '%s_dm_n%d_%d.txt' % (metric, subset, i))\n",
      "                        subset_dm = open(subset_dm_fp, 'w')\n",
      "                        subset_dm.write(pick_dm_subset(open(renamed_dm_fp, 'U'), subset))\n",
      "                        subset_dm.close()\n",
      "            \n",
      "    # Create distance matrices from environmental variables in mapping file. These are independent of sampling depth and metric, so we only need to create them once for each study.\n",
      "    # Again, keyboard is unique in that we cannot easily create a key distance matrix from the mapping file. We'll use one that has been precalculated.\n",
      "    for category in studies[study]['categories']:\n",
      "        env_dm_out_dir = join(out_study_dir, '%s_dm' % category)\n",
      "        env_dm_fp = join(env_dm_out_dir, '%s.txt' % category)\n",
      "        renamed_env_dm_fp = join(out_study_dir, '%s_dm.txt' % category)\n",
      "        !distance_matrix_from_mapping.py -i $map_fp -c $category -o $env_dm_out_dir\n",
      "        !mv $env_dm_fp $renamed_env_dm_fp\n",
      "        !rm -rf $env_dm_out_dir  \n",
      "    \n",
      "    if study == 'keyboard':\n",
      "        key_dm_fp = join(in_study_dir, 'euclidean_key_distances_dm.txt')\n",
      "        !cp $key_dm_fp $out_study_dir/\n",
      "        indiv_dm_fp = join(in_study_dir, 'median_unifrac_individual_distances_dm.txt')\n",
      "        !cp $indiv_dm_fp $out_study_dir/"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 9
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Run gradient analysis methods"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Run each *gradient analysis* statistical method on each distance matrix. These are methods that test out continuous variables such as pH, latitude, etc.."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}