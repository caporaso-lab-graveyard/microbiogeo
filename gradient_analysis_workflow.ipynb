{
 "metadata": {
  "name": "gradient_analysis_workflow"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Define helper functions"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Define various helper functions and their associated tests that will be used in the workflow (and that are not already in QIIME). Run this cell to ensure all required dependencies are installed and setup correctly (e.g. QIIME, numpy) and that all tests pass before running the workflow."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from csv import writer\n",
      "from os.path import join\n",
      "\n",
      "from IPython.parallel import Client\n",
      "\n",
      "from numpy import mean, median\n",
      "\n",
      "# Define functions for manipulating distance matrices.\n",
      "def shuffle_dm(dm_f):\n",
      "    from random import shuffle\n",
      "    from qiime.format import format_distance_matrix\n",
      "    from qiime.parse import parse_distmat\n",
      "    \n",
      "    labels, dm_data = parse_distmat(dm_f)\n",
      "    shuffle(labels)\n",
      "    return format_distance_matrix(labels, dm_data)\n",
      "\n",
      "def pick_dm_subset(dm_f, num_samps):\n",
      "    from random import sample\n",
      "    from qiime.filter import filter_samples_from_distance_matrix\n",
      "    from qiime.parse import parse_distmat\n",
      "    \n",
      "    labels, dm_data = parse_distmat(dm_f)\n",
      "    samp_ids_to_keep = sample(labels, num_samps)\n",
      "    return filter_samples_from_distance_matrix((labels, dm_data), samp_ids_to_keep, negate=True)\n",
      "\n",
      "# TODO These parsing functions are pretty messy... they need to be cleaned up.\n",
      "# Define functions to parse effect size statistics and p-values from the various results files.\n",
      "def parse_mantel_results(results_f):\n",
      "    for line in results_f:\n",
      "        pass\n",
      "    if len(line.strip().split('\\t')) != 7:\n",
      "        raise ValueError(\"Encountered unparsable line: %s\" % line)\n",
      "    es, p_value = map(float, line.strip().split('\\t')[3:5])\n",
      "    if es < -1 or es > 1:\n",
      "        raise ValueError(\"Encountered invalid r value: %.4f\" % es)\n",
      "    if p_value < 0 or p_value > 1:\n",
      "        raise ValueError(\"Encountered invalid p-value: %.4f\" % p_value)\n",
      "    return es, p_value\n",
      "\n",
      "def parse_partial_mantel_results(results_f):\n",
      "    for line in results_f:\n",
      "        pass\n",
      "    if len(line.strip().split('\\t')) != 8:\n",
      "        raise ValueError(\"Encountered unparsable line: %s\" % line)\n",
      "    es, p_value = map(float, line.strip().split('\\t')[4:6])\n",
      "    if es < -1 or es > 1:\n",
      "        raise ValueError(\"Encountered invalid r value: %.4f\" % es)\n",
      "    if p_value < 0 or p_value > 1:\n",
      "        raise ValueError(\"Encountered invalid p-value: %.4f\" % p_value)\n",
      "    return es, p_value\n",
      "\n",
      "def parse_morans_i_results(results_f):\n",
      "    es_next = False\n",
      "    p_value_next = False\n",
      "    \n",
      "    for line in results_f:\n",
      "        if line.startswith('$observed'):\n",
      "            es_next = True\n",
      "        elif line.startswith('$p.value'):\n",
      "            p_value_next = True\n",
      "        elif es_next:\n",
      "            if len(line.strip().split()) != 2:\n",
      "                raise ValueError(\"Encountered unparsable line: %s\" % line)\n",
      "            es = float(line.strip().split()[1])\n",
      "            if es < -1 or es > 1:\n",
      "                raise ValueError(\"Encountered invalid I value: %.4f\" % es)\n",
      "            es_next = False\n",
      "        elif p_value_next:\n",
      "            if len(line.strip().split()) != 2:\n",
      "                raise ValueError(\"Encountered unparsable line: %s\" % line)\n",
      "            p_value = float(line.strip().split()[1])\n",
      "            if p_value < 0 or p_value > 1:\n",
      "                raise ValueError(\"Encountered invalid p-value: %.4f\" % p_value)\n",
      "            p_value_next = False\n",
      "    return es, p_value\n",
      "\n",
      "# TODO Put in checks to ensure we are given a float.\n",
      "def format_p_value_as_asterisk(p_value):\n",
      "    # p_value should be a float\n",
      "    result = 'x'\n",
      "    if p_value <= 0.1:\n",
      "        result = '*'\n",
      "    if p_value <= 0.05:\n",
      "        result += '*'\n",
      "    if p_value <= 0.01:\n",
      "        result += '*'\n",
      "    if p_value <= 0.001:\n",
      "        result += '*'\n",
      "    return result"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Set up workflow parameters"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Configure the variables in this section to control how the workflow will be executed. For example, what studies to analyze, categories of interest, etc.."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "test = False\n",
      "\n",
      "if test:\n",
      "    in_dir = 'test_datasets'\n",
      "    out_dir = 'test_gradient_analysis_output'\n",
      "    tree_fp = join('test_datasets', 'overview', 'rep_set.tre')\n",
      "    depth_descs = ['5_percent', '25_percent', '50_percent']\n",
      "    studies = {\n",
      "               'overview': {\n",
      "                            'depths': [50, 100, 146],\n",
      "                            'categories': ['DOB'],\n",
      "                            'subsets': [3, 4],\n",
      "                            'best_method_env_vars': ['DOB']\n",
      "                           }\n",
      "              }\n",
      "    metrics = ['euclidean', 'bray_curtis']\n",
      "    methods = {\n",
      "               'mantel': parse_mantel_results,\n",
      "               'mantel_corr': None,\n",
      "               'morans_i': parse_morans_i_results,\n",
      "               'partial_mantel': parse_partial_mantel_results,\n",
      "              }\n",
      "    permutations = [99, 999]\n",
      "    num_shuffled = 2\n",
      "    num_subsets = 2\n",
      "else:\n",
      "    in_dir = 'datasets'\n",
      "    out_dir = 'gradient_analysis_output'\n",
      "    tree_fp = join('gg_otus_4feb2011', 'trees', 'gg_97_otus_4feb2011.tre')\n",
      "    depth_descs = ['5_percent', '25_percent', '50_percent']\n",
      "    studies = {\n",
      "               '88_soils': {\n",
      "                            'depths': [400, 580, 660],\n",
      "                            'categories': ['LATITUDE', 'PH'],\n",
      "                            'subsets': [10, 20, 30],\n",
      "                            'best_method_env_vars': ['TOT_ORG_CARB', 'SILT_CLAY', 'ELEVATION', 'SOIL_MOISTURE_DEFICIT', 'CARB_NITRO_RATIO',\n",
      "                                                     'ANNUAL_SEASON_TEMP', 'ANNUAL_SEASON_PRECPT', 'PH', 'CMIN_RATE', 'LONGITUDE', 'LATITUDE']\n",
      "                           }, \n",
      "               'glen_canyon': {\n",
      "                               'depths': [15000, 29000, 53000],\n",
      "                               'categories': ['estimated_years_since_submerged_for_plotting'],\n",
      "                               'subsets': [10, 20, 30],\n",
      "                               'best_method_env_vars': ['sample_pH', 'estimated_years_since_submerged_for_plotting', 'Month', 'Day', 'Year',\n",
      "                                                        'days_since_epoch', 'Hour', 'Replicate', 'DNA.I.D.No.']\n",
      "                              },\n",
      "               'keyboard': {\n",
      "                            'depths': [390, 780, 1015],\n",
      "                            'categories': [],\n",
      "                            'subsets': [10, 20, 30],\n",
      "                            'best_method_env_vars': []\n",
      "                           }\n",
      "              }\n",
      "    metrics = ['euclidean', 'bray_curtis', 'weighted_unifrac', 'unweighted_unifrac']\n",
      "    methods = {\n",
      "               'mantel': parse_mantel_results,\n",
      "               'mantel_corr': None,\n",
      "               'morans_i': parse_morans_i_results,\n",
      "               'partial_mantel': parse_partial_mantel_results\n",
      "              }\n",
      "    #permutations = [99, 999, 9999]\n",
      "    permutations = [99, 999]\n",
      "    num_shuffled = 5\n",
      "    num_subsets = 5"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Generate distance matrices"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Generate distance matrices for each study at even sampling depths that exclude 5%, 25%, and 50% of the samples from the input OTU table (these numbers were calculated beforehand). Generate Euclidean, Bray-Curtis, weighted UniFrac, and unweighted UniFrac distance matrices at each sampling depth using the GreenGenes 97% tree. Also generate several shuffled versions of each distance matrix, which can be used later as negative controls.\n",
      "\n",
      "In addition, generate several subsets of each distance matrix with the specified number of samples, which can be used later to test how the methods perform on different study sizes.\n",
      "\n",
      "The keyboard study is a bit of an exception in that we only want to create a distance matrix that includes samples taken directly from keys (not human subject fingertips) because we want to see if keys that are closer to each other are correlated with community similarity."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# The parameters need to be wrapped in parens in order to work with map.\n",
      "def generate_per_study_depth_dms((study, depth, metrics, categories, subsets, num_shuffled, num_subsets, in_dir, out_dir, tree_fp, shuffle_dm_fn,\n",
      "                                  pick_dm_subset_fn)):\n",
      "    from os.path import join\n",
      "    \n",
      "    in_study_dir = join(in_dir, study)\n",
      "    out_study_dir = join(out_dir, study)\n",
      "    !mkdir $out_study_dir\n",
      "    !cp $in_study_dir/map.txt $out_study_dir/\n",
      "    map_fp = join(out_study_dir, 'map.txt')\n",
      "    \n",
      "    full_otu_fp = join(in_study_dir, 'otu_table.biom')\n",
      "    even_otu_fp = join(out_study_dir, 'otu_table_even%d.biom' % depth)\n",
      "    bdiv_out_dir = join(out_study_dir, 'bdiv_even%d' % depth)\n",
      "    \n",
      "    metrics_param = ','.join(metrics)\n",
      "    !single_rarefaction.py -i $full_otu_fp -o $even_otu_fp -d $depth\n",
      "    !beta_diversity.py -i $even_otu_fp -o $bdiv_out_dir -m $metrics_param -t $tree_fp\n",
      "    \n",
      "    # Rename each file to match QIIME's standard naming conventions of distance matrices. Generate shuffled versions of each distance matrix.\n",
      "    for metric in metrics:\n",
      "        dm_fp = join(bdiv_out_dir, '%s_otu_table_even%d.txt' % (metric, depth))\n",
      "        renamed_dm_fp = join(bdiv_out_dir, '%s_dm.txt' % metric)\n",
      "        \n",
      "        # Filter the keyboard study distance matrix to include only samples taken from keys of subjects M2, M3, and M9.\n",
      "        if study == 'keyboard':\n",
      "            !filter_distance_matrix.py -i $dm_fp -o $renamed_dm_fp -m $map_fp -s 'COMMON_NAME:keyboard;HOST_SUBJECT_ID:M2,M3,M9'\n",
      "            !rm $dm_fp\n",
      "        else:\n",
      "            !mv $dm_fp $renamed_dm_fp\n",
      "        \n",
      "        for i in range(1, num_shuffled + 1):\n",
      "            renamed_dm_f = open(renamed_dm_fp, 'U')\n",
      "            shuffled_dm_fp = join(bdiv_out_dir, '%s_dm_shuffled%d.txt' % (metric, i))\n",
      "            shuffled_dm_f = open(shuffled_dm_fp, 'w')\n",
      "            shuffled_dm_f.write(shuffle_dm_fn(renamed_dm_f))\n",
      "            shuffled_dm_f.close()\n",
      "            renamed_dm_f.close()\n",
      "        \n",
      "        # Create subsets of each non-shuffled distance matrix.\n",
      "        for subset in subsets:\n",
      "            for i in range(1, num_subsets + 1):\n",
      "                    subset_dm_fp = join(bdiv_out_dir, '%s_dm_n%d_%d.txt' % (metric, subset, i))\n",
      "                    subset_dm = open(subset_dm_fp, 'w')\n",
      "                    subset_dm.write(pick_dm_subset_fn(open(renamed_dm_fp, 'U'), subset))\n",
      "                    subset_dm.close()\n",
      "            \n",
      "    # Create distance matrices from environmental variables in mapping file. These are independent of sampling depth and metric, so we only need to create them\n",
      "    # once for each study. Again, keyboard is unique in that we cannot easily create a key distance matrix from the mapping file. We'll use one that has been\n",
      "    # precalculated.\n",
      "    for category in categories:\n",
      "        env_dm_out_dir = join(out_study_dir, '%s_dm' % category)\n",
      "        env_dm_fp = join(env_dm_out_dir, '%s.txt' % category)\n",
      "        renamed_env_dm_fp = join(out_study_dir, '%s_dm.txt' % category)\n",
      "        !distance_matrix_from_mapping.py -i $map_fp -c $category -o $env_dm_out_dir\n",
      "        !mv $env_dm_fp $renamed_env_dm_fp\n",
      "        !rm -rf $env_dm_out_dir  \n",
      "    \n",
      "    if study == 'keyboard':\n",
      "        key_dm_fp = join(in_study_dir, 'euclidean_key_distances_dm.txt')\n",
      "        !cp $key_dm_fp $out_study_dir/\n",
      "        indiv_dm_fp = join(in_study_dir, 'median_unifrac_individual_distances_dm.txt')\n",
      "        !cp $indiv_dm_fp $out_study_dir/\n",
      "        \n",
      "# Process each depth in each study in parallel.\n",
      "c = Client()\n",
      "dview = c[:]\n",
      "dview.block = True\n",
      "\n",
      "!mkdir $out_dir\n",
      "per_study_depths = []\n",
      "for study in studies:\n",
      "    for depth in studies[study]['depths']:\n",
      "        per_study_depths.append((study, depth, metrics, studies[study]['categories'], studies[study]['subsets'], num_shuffled, num_subsets,\n",
      "                                 in_dir, out_dir, tree_fp, shuffle_dm, pick_dm_subset))\n",
      "out = dview.map(generate_per_study_depth_dms, per_study_depths)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Run gradient analysis methods"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Run each *gradient analysis* statistical method on each distance matrix. These are methods that test out continuous variables such as pH, latitude, etc.. Not all methods can be tested on the same variables/inputs. For example, Moran's I cannot be tested on the keyboard study's key distances because it is univariate, and partial Mantel can only be tested on the keyboard study because that is the only study we have a control matrix for."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from os import listdir\n",
      "from os.path import basename, exists, join, splitext\n",
      "from random import shuffle\n",
      "\n",
      "def run_command(cmd):\n",
      "    !$cmd\n",
      "\n",
      "jobs = []\n",
      "for study in studies:\n",
      "    for depth in studies[study]['depths']:\n",
      "        for method in methods:\n",
      "            study_dir = join(out_dir, study)\n",
      "            map_fp = join(study_dir, 'map.txt')\n",
      "            depth_dir = join(study_dir, 'bdiv_even%d' % depth)\n",
      "            dm_wildcard = join(depth_dir, '*_dm*.txt')\n",
      "            dm_fps = !ls $dm_wildcard\n",
      "            \n",
      "            for dm_fp in dm_fps:\n",
      "                for category in studies[study]['categories']:\n",
      "                    for permutation in permutations:\n",
      "                        if method == 'mantel' or method == 'mantel_corr':\n",
      "                            in_dm_fps = '%s,%s' % (dm_fp, join(study_dir, '%s_dm.txt' % category))\n",
      "                            results_dir = join(depth_dir, '%s_%s_%s_%d' % (splitext(basename(dm_fp))[0], method, category, permutation))\n",
      "                            \n",
      "                            # Skip the job if the results dir exists and is not empty. We'll assume it was created from a previous run.\n",
      "                            if not exists(results_dir) or len(listdir(results_dir)) == 0:\n",
      "                                jobs.append('compare_distance_matrices.py --method %s -n %d -i %s -o %s' % (method, permutation, in_dm_fps, results_dir))\n",
      "                    \n",
      "                    # Moran's I does not accept a number of permutations.\n",
      "                    if method == 'morans_i':\n",
      "                        results_dir = join(depth_dir, '%s_%s_%s' % (splitext(basename(dm_fp))[0], method, category))\n",
      "                        \n",
      "                        if not exists(results_dir) or len(listdir(results_dir)) == 0:\n",
      "                            jobs.append('compare_categories.py --method %s -i %s -m %s -c %s -o %s' % (method, dm_fp, map_fp, category, results_dir))\n",
      "        \n",
      "            if study == 'keyboard':\n",
      "                for dm_fp in dm_fps:\n",
      "                    for permutation in permutations:\n",
      "                        in_dm_fps = '%s,%s' % (dm_fp, join(study_dir, 'euclidean_key_distances_dm.txt'))\n",
      "                        \n",
      "                        if method == 'mantel' or method == 'mantel_corr':\n",
      "                            results_dir = join(depth_dir, '%s_%s_%s_%d' % (splitext(basename(dm_fp))[0], method, 'key_distance', permutation))\n",
      "                            \n",
      "                            if not exists(results_dir) or len(listdir(results_dir)) == 0:\n",
      "                                jobs.append('compare_distance_matrices.py --method %s -n %d -i %s -o %s' % (method, permutation, in_dm_fps, results_dir))\n",
      "                        elif method == 'partial_mantel':\n",
      "                            control_dm_fp = join(study_dir, 'median_unifrac_individual_distances_dm.txt')\n",
      "                            results_dir = join(depth_dir, '%s_%s_%s_%d' % (splitext(basename(dm_fp))[0], method, 'key_distance', permutation))\n",
      "                            \n",
      "                            if not exists(results_dir) or len(listdir(results_dir)) == 0:\n",
      "                                jobs.append('compare_distance_matrices.py --method %s -n %d -i %s -o %s -c %s' % (method, permutation, in_dm_fps, results_dir, control_dm_fp))\n",
      "\n",
      "# Process each script run in parallel.\n",
      "c = Client()\n",
      "dview = c[:]\n",
      "dview.block = True\n",
      "\n",
      "shuffle(jobs)\n",
      "out = dview.map(run_command, jobs)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Run BEST method"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "BEST is very different from the rest of the methods in terms of input and output, so run it separately here."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def run_per_study_depth_best_analysis((study, depth, env_vars, out_dir)):\n",
      "    from os.path import basename, join, splitext\n",
      "    \n",
      "    if len(env_vars) > 0:\n",
      "        study_dir = join(out_dir, study)\n",
      "        map_fp = join(study_dir, 'map.txt')\n",
      "        depth_dir = join(study_dir, 'bdiv_even%d' % depth)\n",
      "        dm_wildcard = join(depth_dir, '*_dm*.txt')\n",
      "        dm_fps = !ls $dm_wildcard\n",
      "        env_vars_str = ','.join(env_vars)\n",
      "        \n",
      "        for dm_fp in dm_fps:\n",
      "            results_dir = join(depth_dir, '%s_%s' % (splitext(basename(dm_fp))[0], 'best'))\n",
      "            !compare_categories.py --method 'best' -i $dm_fp -m $map_fp -c $env_vars_str -o $results_dir\n",
      "\n",
      "# Process BEST at each depth in each study in parallel.\n",
      "c = Client()\n",
      "dview = c[:]\n",
      "dview.block = True\n",
      "\n",
      "per_study_depths = []\n",
      "for study in studies:\n",
      "    for depth in studies[study]['depths']:\n",
      "        per_study_depths.append((study, depth, studies[study]['best_method_env_vars'], out_dir))\n",
      "out = dview.map(run_per_study_depth_best_analysis, per_study_depths)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Collate results"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Parse and collect the effect size statistics and p-values from each of the tests that were run. The resulting data structure can then be used to format result tables.\n",
      "\n",
      "Mantel correlogram is hard to summarize because it produces a correlogram, and many Mantel statistics for each distance class. We'll need to look at those results by hand and summarize them in the paper. The same holds for BEST: though it does not create a visual plot, it does not provide p-values. It mainly tells you which environmental variables best correlate with the community data."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "results = {}\n",
      "for depth_idx, depth_desc in enumerate(depth_descs):\n",
      "    depth_res = {}\n",
      "    for metric in metrics:\n",
      "        metric_res = {}\n",
      "        for method, res_parsing_fn in methods.items():\n",
      "            if method == 'mantel_corr':\n",
      "                # Completely ignore Mantel correlogram (for now at least).\n",
      "                continue\n",
      "                \n",
      "            method_res = {}\n",
      "            for study in studies:\n",
      "                study_res = {}\n",
      "                \n",
      "                # Figure out what our actual depth is for the study and what distance matrix subsets we used.\n",
      "                depth = studies[study]['depths'][depth_idx]\n",
      "                subsets = studies[study]['subsets']\n",
      "                \n",
      "                for category in studies[study]['categories']:\n",
      "                    category_res = {}\n",
      "                    \n",
      "                    if method == 'mantel' or method == 'morans_i':\n",
      "                        full_ess = []\n",
      "                        full_p_vals = []\n",
      "                        shuff_ess = []\n",
      "                        shuff_p_vals = []\n",
      "                        \n",
      "                        if method == 'mantel':\n",
      "                            # TODO This next part is pretty messy... it needs to get cleaned up.\n",
      "                            for permutation in permutations:\n",
      "                                # Collect results for full distance matrices.\n",
      "                                full_res_f = open(join(out_dir, study, 'bdiv_even%d' % depth, '%s_dm_%s_%s_%d' % (metric, method, category, permutation), '%s_results.txt' % method), 'U')\n",
      "                                full_es, full_p_val = res_parsing_fn(full_res_f)\n",
      "                                full_res_f.close()\n",
      "                                full_ess.append(full_es)\n",
      "                                full_p_vals.append(full_p_val)\n",
      "                                \n",
      "                                # Collect results for shuffled distance matrices.\n",
      "                                avg_shuff_ess = []\n",
      "                                avg_shuff_p_vals = []\n",
      "                                for shuff_num in range(1, num_shuffled + 1):\n",
      "                                    shuff_res_f = open(join(out_dir, study, 'bdiv_even%d' % depth, '%s_dm_shuffled%d_%s_%s_%d' % (metric, shuff_num, method, category, permutation), '%s_results.txt' % method), 'U')\n",
      "                                    shuff_es, shuff_p_val = res_parsing_fn(shuff_res_f)\n",
      "                                    shuff_res_f.close()\n",
      "                                    avg_shuff_ess.append(shuff_es)\n",
      "                                    avg_shuff_p_vals.append(shuff_p_val)\n",
      "                                shuff_ess.append(median(avg_shuff_ess))\n",
      "                                shuff_p_vals.append(median(avg_shuff_p_vals))\n",
      "                        elif method == 'morans_i':\n",
      "                            full_res_f = open(join(out_dir, study, 'bdiv_even%d' % depth, '%s_dm_%s_%s' % (metric, method, category), '%s_results.txt' % method), 'U')\n",
      "                            full_es, full_p_val = res_parsing_fn(full_res_f)\n",
      "                            full_res_f.close()\n",
      "                            full_ess.append(full_es)\n",
      "                            full_p_vals.append(full_p_val)\n",
      "                            \n",
      "                            avg_shuff_ess = []\n",
      "                            avg_shuff_p_vals = []\n",
      "                            for shuff_num in range(1, num_shuffled + 1):\n",
      "                                shuff_res_f = open(join(out_dir, study, 'bdiv_even%d' % depth, '%s_dm_shuffled%d_%s_%s' % (metric, shuff_num, method, category), '%s_results.txt' % method), 'U')\n",
      "                                shuff_es, shuff_p_val = res_parsing_fn(shuff_res_f)\n",
      "                                shuff_res_f.close()\n",
      "                                avg_shuff_ess.append(shuff_es)\n",
      "                                avg_shuff_p_vals.append(shuff_p_val)\n",
      "                            shuff_ess.append(median(avg_shuff_ess))\n",
      "                            shuff_p_vals.append(median(avg_shuff_p_vals))\n",
      "                                \n",
      "                        if len(set(full_ess)) != 1 or len(set(shuff_ess)) != 1:\n",
      "                            raise ValueError(\"The effect size statistics were not the same for different numbers of permutations. Something went wrong...\")\n",
      "                        for p_val in full_p_vals:\n",
      "                            if p_val < 0 or p_val > 1:\n",
      "                                raise ValueError(\"Encountered invalid p-value: %.4f\" % p_val)\n",
      "                        for p_val in shuff_p_vals:\n",
      "                            if p_val < 0 or p_val > 1:\n",
      "                                raise ValueError(\"Encountered invalid p-value: %.4f\" % p_val)\n",
      "                        category_res['full'] = (full_ess[0], full_p_vals)\n",
      "                        category_res['shuffled'] = (shuff_ess[0], shuff_p_vals)\n",
      "                            \n",
      "                        # Collect results for distance matrix subsets.\n",
      "                        ss_ess = []\n",
      "                        ss_p_vals = []\n",
      "                        for subset in subsets:\n",
      "                            gs_ess = []\n",
      "                            gs_p_vals = []\n",
      "                            \n",
      "                            if method == 'mantel':\n",
      "                                for permutation in permutations:\n",
      "                                    avg_ss_ess = []\n",
      "                                    avg_ss_p_vals = []\n",
      "                                    for ss_num in range(1, num_subsets + 1):\n",
      "                                        ss_res_f = open(join(out_dir, study, 'bdiv_even%d' % depth, '%s_dm_n%d_%d_%s_%s_%d' % (metric, subset, ss_num, method, category, permutation),\n",
      "                                                             '%s_results.txt' % method), 'U')\n",
      "                                        ss_es, ss_p_val = res_parsing_fn(ss_res_f)\n",
      "                                        ss_res_f.close()\n",
      "                                        avg_ss_ess.append(ss_es)\n",
      "                                        avg_ss_p_vals.append(ss_p_val)\n",
      "                                    gs_ess.append(median(avg_ss_ess))\n",
      "                                    gs_p_vals.append(median(avg_ss_p_vals))\n",
      "                            elif method == 'morans_i':\n",
      "                                avg_ss_ess = []\n",
      "                                avg_ss_p_vals = []\n",
      "                                for ss_num in range(1, num_subsets + 1):\n",
      "                                    ss_res_f = open(join(out_dir, study, 'bdiv_even%d' % depth, '%s_dm_n%d_%d_%s_%s' % (metric, subset, ss_num, method, category),\n",
      "                                                         '%s_results.txt' % method), 'U')\n",
      "                                    ss_es, ss_p_val = res_parsing_fn(ss_res_f)\n",
      "                                    ss_res_f.close()\n",
      "                                    avg_ss_ess.append(ss_es)\n",
      "                                    avg_ss_p_vals.append(ss_p_val)\n",
      "                                gs_ess.append(median(avg_ss_ess))\n",
      "                                gs_p_vals.append(median(avg_ss_p_vals))\n",
      "                            \n",
      "                            if len(set(gs_ess)) != 1:\n",
      "                                raise ValueError(\"The effect size statistics were not the same for different numbers of permutations. Something went wrong...\")\n",
      "                            for p_val in gs_p_vals:\n",
      "                                if p_val < 0 or p_val > 1:\n",
      "                                    raise ValueError(\"Encountered invalid p-value: %.4f\" % p_val)\n",
      "                            ss_ess.append(gs_ess[0])\n",
      "                            ss_p_vals.append(gs_p_vals)\n",
      "                        \n",
      "                        if len(ss_ess) != len(ss_p_vals):\n",
      "                            raise ValueError(\"We don't have the same number of effect size statistics as p-values. Something went wrong...\")\n",
      "                        category_res['subsampled'] = (ss_ess, ss_p_vals)\n",
      "                    \n",
      "                    # Add the category results. Will be an empty dictionary if the method (such as partial Mantel) was not applicable.\n",
      "                    study_res[category] = category_res\n",
      "                \n",
      "                if study == 'keyboard':\n",
      "                    category = 'key_distance'\n",
      "                    category_res = {}\n",
      "                    \n",
      "                    if method == 'mantel' or method == 'partial_mantel':\n",
      "                        full_ess = []\n",
      "                        full_p_vals = []\n",
      "                        shuff_ess = []\n",
      "                        shuff_p_vals = []\n",
      "                        \n",
      "                        for permutation in permutations:\n",
      "                            full_res_f = open(join(out_dir, study, 'bdiv_even%d' % depth, '%s_dm_%s_%s_%d' % (metric, method, category, permutation), '%s_results.txt' % method), 'U')\n",
      "                            full_es, full_p_val = res_parsing_fn(full_res_f)\n",
      "                            full_res_f.close()\n",
      "                            full_ess.append(full_es)\n",
      "                            full_p_vals.append(full_p_val)\n",
      "                            \n",
      "                            avg_shuff_ess = []\n",
      "                            avg_shuff_p_vals = []\n",
      "                            for shuff_num in range(1, num_shuffled + 1):\n",
      "                                shuff_res_f = open(join(out_dir, study, 'bdiv_even%d' % depth, '%s_dm_shuffled%d_%s_%s_%d' % (metric, shuff_num, method, category, permutation), '%s_results.txt' % method), 'U')\n",
      "                                shuff_es, shuff_p_val = res_parsing_fn(shuff_res_f)\n",
      "                                shuff_res_f.close()\n",
      "                                avg_shuff_ess.append(shuff_es)\n",
      "                                avg_shuff_p_vals.append(shuff_p_val)\n",
      "                            shuff_ess.append(median(avg_shuff_ess))\n",
      "                            shuff_p_vals.append(median(avg_shuff_p_vals))\n",
      "                            \n",
      "                        if len(set(full_ess)) != 1 or len(set(shuff_ess)) != 1:\n",
      "                            raise ValueError(\"The effect size statistics were not the same for different numbers of permutations. Something went wrong...\")\n",
      "                        for p_val in full_p_vals:\n",
      "                            if p_val < 0 or p_val > 1:\n",
      "                                raise ValueError(\"Encountered invalid p-value: %.4f\" % p_val)\n",
      "                        for p_val in shuff_p_vals:\n",
      "                            if p_val < 0 or p_val > 1:\n",
      "                                raise ValueError(\"Encountered invalid p-value: %.4f\" % p_val)\n",
      "                        category_res['full'] = (full_ess[0], full_p_vals)\n",
      "                        category_res['shuffled'] = (shuff_ess[0], shuff_p_vals)\n",
      "                        \n",
      "                        # Collect results for distance matrix subsets.\n",
      "                        ss_ess = []\n",
      "                        ss_p_vals = []\n",
      "                        for subset in subsets:\n",
      "                            gs_ess = []\n",
      "                            gs_p_vals = []\n",
      "                            \n",
      "                            for permutation in permutations:\n",
      "                                avg_ss_ess = []\n",
      "                                avg_ss_p_vals = []\n",
      "                                for ss_num in range(1, num_subsets + 1):\n",
      "                                    ss_res_f = open(join(out_dir, study, 'bdiv_even%d' % depth, '%s_dm_n%d_%d_%s_%s_%d' % (metric, subset, ss_num, method, category, permutation),\n",
      "                                                         '%s_results.txt' % method), 'U')\n",
      "                                    ss_es, ss_p_val = res_parsing_fn(ss_res_f)\n",
      "                                    ss_res_f.close()\n",
      "                                    avg_ss_ess.append(ss_es)\n",
      "                                    avg_ss_p_vals.append(ss_p_val)\n",
      "                                gs_ess.append(median(avg_ss_ess))\n",
      "                                gs_p_vals.append(median(avg_ss_p_vals))\n",
      "                        \n",
      "                            if len(set(gs_ess)) != 1:\n",
      "                                raise ValueError(\"The effect size statistics were not the same for different numbers of permutations. Something went wrong...\")\n",
      "                            for p_val in gs_p_vals:\n",
      "                                if p_val < 0 or p_val > 1:\n",
      "                                    raise ValueError(\"Encountered invalid p-value: %.4f\" % p_val)\n",
      "                            ss_ess.append(gs_ess[0])\n",
      "                            ss_p_vals.append(gs_p_vals)\n",
      "                    \n",
      "                        if len(ss_ess) != len(ss_p_vals):\n",
      "                            raise ValueError(\"We don't have the same number of effect size statistics as p-values. Something went wrong...\")\n",
      "                        category_res['subsampled'] = (ss_ess, ss_p_vals)\n",
      "                        \n",
      "                    study_res[category] = category_res\n",
      "                method_res[study] = study_res\n",
      "            metric_res[method] = method_res\n",
      "        depth_res[metric] = metric_res\n",
      "    results[depth_desc] = depth_res"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Build results summary tables"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Build tables to summarize the results of the statistical methods. These tables will be in TSV format so that they can be easily imported into Excel for viewing and cleanup for publication."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for depth_desc, depth_res in results.items():\n",
      "    for metric, metric_res in depth_res.items():\n",
      "        constructed_header = False\n",
      "        header = ['Method']\n",
      "        rows = []\n",
      "        for method, method_res in sorted(metric_res.items()):\n",
      "            row = ['%s' % method]\n",
      "            for study, study_res in sorted(method_res.items()):\n",
      "                for category, category_res in sorted(study_res.items()):\n",
      "                    # Format full results.\n",
      "                    header.append('%s\\r%s' % (study, category))\n",
      "                    if len(category_res) > 0:\n",
      "                        row.append('%.2f; %s' % (category_res['full'][0], ', '.join(map(format_p_value_as_asterisk, category_res['full'][1]))))\n",
      "                    else:\n",
      "                        row.append('N/A')\n",
      "                    \n",
      "                    # Format shuffled results.\n",
      "                    header.append('%s\\r%s (shuffled)' % (study, category))\n",
      "                    if len(category_res) > 0:\n",
      "                        row.append('%.2f; %s' % (category_res['shuffled'][0], ', '.join(map(format_p_value_as_asterisk, category_res['shuffled'][1]))))\n",
      "                    else:\n",
      "                        row.append('N/A')\n",
      "                    \n",
      "                    # Format subsampled results.\n",
      "                    header.append('%s\\r%s (subsampled)' % (study, category))\n",
      "                    if len(category_res) > 0:\n",
      "                        cell = ['%.2f; %s' % (es, ', '.join(map(format_p_value_as_asterisk, p_vals))) for es, p_vals in zip(*category_res['subsampled'])]\n",
      "                        row.append('\\r'.join(cell))\n",
      "                    else:\n",
      "                        row.append('N/A')\n",
      "                        \n",
      "            if not constructed_header:\n",
      "                rows.append(header[:])\n",
      "                constructed_header = True\n",
      "            rows.append(row)\n",
      "        with open(join(out_dir, 'gradient_analysis_method_comparison_table_%s_%s.txt' % (depth_desc, metric)), 'wb') as out_f:\n",
      "            # We use \\r so that we can force linebreaks within cells when imported into Excel. Not sure if this will work with other spreadsheet programs such as Open Office.\n",
      "            tsv_writer = writer(out_f, delimiter='\\t', lineterminator='\\r')\n",
      "            tsv_writer.writerows(rows)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 8
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Gradient subset testing for 88 soils pH"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Specifically test Mantel and Moran's I on subsets of the original distance matrix. This time the subsets will not be randomly chosen, but instead samples will be chosen along the gradient for each subset (which is what a researcher might do instead of randomly picking samples in order to test a gradient). A plot will be generated with subset size on the x-axis and test statistic on the y-axis. This will allow us to see if there is a cutoff/threshold for gradient detection based on the number of samples."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from __future__ import division\n",
      "from math import ceil\n",
      "from os.path import join\n",
      "from matplotlib.pyplot import figure, plot\n",
      "from qiime.filter import filter_samples_from_distance_matrix\n",
      "from qiime.parse import parse_distmat, parse_mapping_file_to_dict\n",
      "\n",
      "# Algo: read in mapping file\n",
      "# get list of (samp_id, ph)\n",
      "# sort by ph (ascending)\n",
      "# pick n samples evenly along ph range\n",
      "# filter full dm to include only those samples\n",
      "# run tests over these matrices\n",
      "# parse and collate results\n",
      "# generate plot\n",
      "samp_sizes = [5, 10, 20, 40, 60, 80]\n",
      "gradient = 'LATITUDE'\n",
      "out_dir = '88_soils_%s_gradient_analysis' % gradient\n",
      "\n",
      "mdm, _ = parse_mapping_file_to_dict(open('gradient_analysis_output/88_soils/map.txt', 'U'))\n",
      "phs = [(samp_id, float(metadata[gradient])) for samp_id, metadata in mdm.items()]\n",
      "phs.sort(key=lambda ph: ph[1])\n",
      "\n",
      "dm_labels, dm_data = parse_distmat(open('gradient_analysis_output/88_soils/bdiv_even400/unweighted_unifrac_dm.txt', 'U'))\n",
      "!mkdir $out_dir\n",
      "\n",
      "test_stats = []\n",
      "for samp_size in samp_sizes:\n",
      "    # From http://stackoverflow.com/a/9873935\n",
      "    samp_ids_to_keep = [phs[int(ceil(i * len(phs) / samp_size))][0] for i in range(samp_size)]\n",
      "    assert len(samp_ids_to_keep) == samp_size, \"%d != %d\" % (len(samp_ids_to_keep), samp_size)\n",
      "\n",
      "    subset_dm_fp = join(out_dir, 'unweighted_unifrac_dm_size_%d.txt' % samp_size)\n",
      "    subset_dm_f = open(subset_dm_fp, 'w')\n",
      "    subset_dm_f.write(filter_samples_from_distance_matrix((dm_labels, dm_data), samp_ids_to_keep, negate=True))\n",
      "    subset_dm_f.close()\n",
      "    \n",
      "    in_dm_fps = '%s,%s' % (subset_dm_fp, join('gradient_analysis_output/88_soils/%s_dm.txt' % gradient))\n",
      "    results_dir = join(out_dir, 'unweighted_unifrac_dm_size_%d_mantel_999' % samp_size)\n",
      "    !compare_distance_matrices.py --method mantel -n 999 -i $in_dm_fps -o $results_dir\n",
      "    \n",
      "    test_stat, p_val = parse_mantel_results(open(join(results_dir, 'mantel_results.txt'), 'U'))\n",
      "    test_stats.append(test_stat)\n",
      "\n",
      "fig = figure()\n",
      "ax = fig.add_subplot(111)\n",
      "plot(samp_sizes, test_stats)\n",
      "fig.savefig(join(out_dir, 'gradient_analysis_plot.pdf'), format='pdf')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 11
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# For testing...\n",
      "f1 = \"\"\"# Number of entries refers to the number of rows (or cols) retained in each\n",
      "# distance matrix after filtering the distance matrices to include only those\n",
      "# samples that were in both distance matrices. p-value contains the correct\n",
      "# number of significant digits.\n",
      "DM1\tDM2\tNumber of entries\tMantel r statistic\tp-value\tNumber of permutations\tTail type\n",
      "/Users/jrideout/analysis/overview_tutorial/wf_bdiv_even146/unweighted_unifrac_dm.txt\t/Users/jrideout/analysis/overview_tutorial/wf_bdiv_even146/unweighted_unifrac_dm.txt\t9\t1.00000\t0.01\t100\ttwo sided\"\"\".split('\\n')\n",
      "\n",
      "a, b = parse_mantel_results(f1)\n",
      "print a\n",
      "print b\n",
      "print ''\n",
      "\n",
      "f1 = \"\"\"# Number of entries refers to the number of rows (or cols) retained in each\n",
      "# distance matrix after filtering the distance matrices to include only those\n",
      "# samples that were in both distance matrices. p-value contains the correct\n",
      "# number of significant digits.\n",
      "DM1\tDM2\tCDM\tNumber of entries\tMantel r statistic\tp-value\tNumber of permutations\tTail type\n",
      "/Users/jrideout/analysis/overview_tutorial/wf_bdiv_even146/unweighted_unifrac_dm.txt\t/Users/jrideout/analysis/overview_tutorial/wf_bdiv_even146/unweighted_unifrac_dm.txt\t/Users/jrideout/analysis/overview_tutorial/wf_bdiv_even146/unweighted_unifrac_dm.txt\t9\t0.50000\t0.01\t100\tgreater\"\"\".split('\\n')\n",
      "\n",
      "a, b = parse_partial_mantel_results(f1)\n",
      "print a\n",
      "print b\n",
      "print ''\n",
      "\n",
      "f1 = \"\"\"$observed\n",
      "[1] -0.06005486\n",
      "\n",
      "$expected\n",
      "[1] -0.125\n",
      "\n",
      "$sd\n",
      "[1] 0.01590547\n",
      "\n",
      "$p.value\n",
      "[1] 4.442088e-05\n",
      "\"\"\".split('\\n')\n",
      "\n",
      "a, b = parse_morans_i_results(f1)\n",
      "print a\n",
      "print b\n",
      "print ''"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "1.0\n",
        "0.01\n",
        "\n",
        "0.5\n",
        "0.01\n",
        "\n",
        "-0.06005486\n",
        "4.442088e-05\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 9
    }
   ],
   "metadata": {}
  }
 ]
}