{
 "metadata": {
  "name": "gradient_analysis_workflow"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Define helper functions"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Define various helper functions and their associated tests that will be used in the workflow (and that are not already in QIIME). Run this cell to ensure all required dependencies are installed and setup correctly (e.g. QIIME, numpy) and that all tests pass before running the workflow."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from os.path import join\n",
      "\n",
      "from IPython.parallel import Client\n",
      "\n",
      "# Define functions for manipulating distance matrices.\n",
      "def shuffle_dm(dm_f):\n",
      "    from random import shuffle\n",
      "    from qiime.format import format_distance_matrix\n",
      "    from qiime.parse import parse_distmat\n",
      "    \n",
      "    labels, dm_data = parse_distmat(dm_f)\n",
      "    shuffle(labels)\n",
      "    return format_distance_matrix(labels, dm_data)\n",
      "\n",
      "def pick_dm_subset(dm_f, num_samps):\n",
      "    from random import sample\n",
      "    from qiime.filter import filter_samples_from_distance_matrix\n",
      "    from qiime.parse import parse_distmat\n",
      "    \n",
      "    labels, dm_data = parse_distmat(dm_f)\n",
      "    samp_ids_to_keep = sample(labels, num_samps)\n",
      "    return filter_samples_from_distance_matrix((labels, dm_data), samp_ids_to_keep, negate=True)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 16
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Set up workflow parameters"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Configure the variables in this section to control how the workflow will be executed. For example, what studies to analyze, categories of interest, etc.."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "test = True\n",
      "\n",
      "if test:\n",
      "    in_dir = 'test_datasets'\n",
      "    out_dir = 'test_gradient_analysis_output'\n",
      "    tree_fp = join('test_datasets', 'overview', 'rep_set.tre')\n",
      "    depth_descs = ['5_percent', '25_percent', '50_percent']\n",
      "    studies = {\n",
      "               'overview': {\n",
      "                            'depths': [50, 100, 146],\n",
      "                            'categories': ['DOB'],\n",
      "                            'subsets': [3, 4]\n",
      "                           }\n",
      "              }\n",
      "    metrics = ['euclidean', 'bray_curtis']\n",
      "    #metrics = ['euclidean', 'bray_curtis', 'weighted_unifrac', 'unweighted_unifrac']\n",
      "    permutations = [99, 999]\n",
      "    num_shuffled = 2\n",
      "    num_subsets = 2\n",
      "    jobs_to_start = 2\n",
      "else:\n",
      "    in_dir = 'datasets'\n",
      "    out_dir = 'gradient_analysis_output'\n",
      "    tree_fp = join('gg_otus_4feb2011', 'trees', 'gg_97_otus_4feb2011.tre')\n",
      "    depth_descs = ['5_percent', '25_percent', '50_percent']\n",
      "    studies = {\n",
      "               '88_soils': {\n",
      "                            'depths': [400, 580, 660],\n",
      "                            'categories': ['LATITUDE', 'PH'],\n",
      "                            'subsets': [5, 10, 20, 40]\n",
      "                           }, \n",
      "               'glen_canyon': {\n",
      "                               'depths': [15000, 29000, 53000],\n",
      "                               'categories': ['estimated_years_since_submerged_for_plotting'],\n",
      "                               'subsets': [5, 10, 20, 40]\n",
      "                              },\n",
      "               'keyboard': {\n",
      "                            'depths': [390, 780, 1015],\n",
      "                            'categories': [],\n",
      "                            'subsets': [5, 10, 20, 40]\n",
      "                           }\n",
      "              }\n",
      "    metrics = ['euclidean', 'bray_curtis', 'weighted_unifrac', 'unweighted_unifrac']\n",
      "    permutations = [99, 999, 9999]\n",
      "    num_shuffled = 3\n",
      "    num_subsets = 3\n",
      "    jobs_to_start = 2"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 17
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Generate distance matrices"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Generate distance matrices for each study at even sampling depths that exclude 5%, 25%, and 50% of the samples from the input OTU table (these numbers were calculated beforehand). Generate Euclidean, Bray-Curtis, weighted UniFrac, and unweighted UniFrac distance matrices at each sampling depth using the GreenGenes 97% tree. Also generate several shuffled versions of each distance matrix, which can be used later as negative controls.\n",
      "\n",
      "In addition, generate several subsets of each distance matrix with the specified number of samples, which can be used later to test how the methods perform on different study sizes.\n",
      "\n",
      "The keyboard study is a bit of an exception in that we only want to create a distance matrix that includes samples taken directly from keys (not human subject fingertips) because we want to see if keys that are closer to each other are correlated with community similarity."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# The parameters need to be wrapped in parens in order to work with map.\n",
      "def generate_per_study_dms((study, depths, metrics, categories, subsets, num_shuffled, num_subsets, in_dir, out_dir, tree_fp, jobs_to_start, shuffle_dm_fn, pick_dm_subset_fn)):\n",
      "    from os.path import join\n",
      "    \n",
      "    in_study_dir = join(in_dir, study)\n",
      "    out_study_dir = join(out_dir, study)\n",
      "    !mkdir $out_study_dir\n",
      "    !cp $in_study_dir/map.txt $out_study_dir/\n",
      "    map_fp = join(out_study_dir, 'map.txt')\n",
      "    \n",
      "    for depth in depths:\n",
      "        full_otu_fp = join(in_study_dir, 'otu_table.biom')\n",
      "        even_otu_fp = join(out_study_dir, 'otu_table_even%d.biom' % depth)\n",
      "        bdiv_out_dir = join(out_study_dir, 'bdiv_even%d' % depth)\n",
      "        \n",
      "        metrics_param = ','.join(metrics)\n",
      "        !single_rarefaction.py -i $full_otu_fp -o $even_otu_fp -d $depth\n",
      "        !parallel_beta_diversity.py -i $even_otu_fp -o $bdiv_out_dir -m $metrics_param -T -t $tree_fp -O $jobs_to_start\n",
      "        \n",
      "        # Rename each file to match QIIME's standard naming conventions of distance matrices. Generate shuffled versions of each distance matrix.\n",
      "        for metric in metrics:\n",
      "            dm_fp = join(bdiv_out_dir, '%s_otu_table_even%d.txt' % (metric, depth))\n",
      "            renamed_dm_fp = join(bdiv_out_dir, '%s_dm.txt' % metric)\n",
      "            \n",
      "            # Filter the keyboard study distance matrix to include only samples taken from keys of subjects M2, M3, and M9.\n",
      "            if study == 'keyboard':\n",
      "                !filter_distance_matrix.py -i $dm_fp -o $renamed_dm_fp -m $map_fp -s 'COMMON_NAME:keyboard;HOST_SUBJECT_ID:M2,M3,M9'\n",
      "                !rm $dm_fp\n",
      "            else:\n",
      "                !mv $dm_fp $renamed_dm_fp\n",
      "            \n",
      "            for i in range(1, num_shuffled + 1):\n",
      "                renamed_dm_f = open(renamed_dm_fp, 'U')\n",
      "                shuffled_dm_fp = join(bdiv_out_dir, '%s_dm_shuffled%d.txt' % (metric, i))\n",
      "                shuffled_dm_f = open(shuffled_dm_fp, 'w')\n",
      "                shuffled_dm_f.write(shuffle_dm_fn(renamed_dm_f))\n",
      "                shuffled_dm_f.close()\n",
      "                renamed_dm_f.close()\n",
      "            \n",
      "            # Create subsets of each non-shuffled distance matrix.\n",
      "            for subset in subsets:\n",
      "                for i in range(1, num_subsets + 1):\n",
      "                        subset_dm_fp = join(bdiv_out_dir, '%s_dm_n%d_%d.txt' % (metric, subset, i))\n",
      "                        subset_dm = open(subset_dm_fp, 'w')\n",
      "                        subset_dm.write(pick_dm_subset_fn(open(renamed_dm_fp, 'U'), subset))\n",
      "                        subset_dm.close()\n",
      "            \n",
      "    # Create distance matrices from environmental variables in mapping file. These are independent of sampling depth and metric, so we only need to create them once for each study.\n",
      "    # Again, keyboard is unique in that we cannot easily create a key distance matrix from the mapping file. We'll use one that has been precalculated.\n",
      "    for category in categories:\n",
      "        env_dm_out_dir = join(out_study_dir, '%s_dm' % category)\n",
      "        env_dm_fp = join(env_dm_out_dir, '%s.txt' % category)\n",
      "        renamed_env_dm_fp = join(out_study_dir, '%s_dm.txt' % category)\n",
      "        !distance_matrix_from_mapping.py -i $map_fp -c $category -o $env_dm_out_dir\n",
      "        !mv $env_dm_fp $renamed_env_dm_fp\n",
      "        !rm -rf $env_dm_out_dir  \n",
      "    \n",
      "    if study == 'keyboard':\n",
      "        key_dm_fp = join(in_study_dir, 'euclidean_key_distances_dm.txt')\n",
      "        !cp $key_dm_fp $out_study_dir/\n",
      "        indiv_dm_fp = join(in_study_dir, 'median_unifrac_individual_distances_dm.txt')\n",
      "        !cp $indiv_dm_fp $out_study_dir/\n",
      "        \n",
      "# Process each study in parallel.\n",
      "c = Client()\n",
      "dview = c[:]\n",
      "dview.block = True\n",
      "\n",
      "!mkdir $out_dir\n",
      "out = dview.map(generate_per_study_dms, [(study, studies[study]['depths'], metrics, studies[study]['categories'], studies[study]['subsets'], num_shuffled, num_subsets,\n",
      "                                          in_dir, out_dir, tree_fp, jobs_to_start, shuffle_dm, pick_dm_subset) for study in studies])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 18
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Run gradient analysis methods"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Run each *gradient analysis* statistical method on each distance matrix. These are methods that test out continuous variables such as pH, latitude, etc.."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}